{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equal-principle",
   "metadata": {
    "id": "referenced-gabriel"
   },
   "source": [
    "# Predictive Models for GHG Scope 1\n",
    "\n",
    "In this notebook, we present the four predictive models we worked on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-vintage",
   "metadata": {
    "id": "metric-country"
   },
   "source": [
    "## Feauture Engineering\n",
    "\n",
    "Before coding the models, we will create two new variables in our dataset. \n",
    "\n",
    "1. Missing_GHG - Boolean indicating if the stock had a missing value in that year (1 = True / 0 = False)\n",
    "2. Utilities - Boolean indicating if it is a Utility company (1 = True / 0 = False)\n",
    "3. Time Trend - Cumulative value (0,1,2,3) for each year the stock is in the dataset.\n",
    "\n",
    "The second variable will be used in the Model #3 and #4\n",
    "\n",
    "Also, we will create a function to help us apply the k-fold cross validations to any number of splits and to whichever model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-maine",
   "metadata": {
    "id": "abandoned-latitude"
   },
   "source": [
    "#### Missing_GHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-allen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PV-FA_UhBI12",
    "outputId": "39cb95c5-1b6b-448c-cb76-45175e2329fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE\n",
    "from datetime import datetime, date\n",
    "from mpl_toolkits.mplot3d import Axes3D # Visualize the Data for Multiple Linear Regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stocks = pd.read_csv(\"/Users/maralinetorres/Documents/GitHub/Predicting-Environmental-and-Social-Actions/Datasets/pilot_stocks.csv\")\n",
    "sectors = pd.read_csv(\"/Users/maralinetorres/Documents/GitHub/Predicting-Environmental-and-Social-Actions/Datasets/52_tickers_sectors.csv\")\n",
    "\n",
    "stocks['Missing_GHG'] = np.where(stocks['GHG Scope 1'].isna(), 1, 0)\n",
    "stocks.loc[stocks['GHG Scope 1'].isna(),['GHG Scope 1','Missing_GHG']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-fourth",
   "metadata": {
    "id": "present-nurse"
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-exclusion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "posted-blair",
    "outputId": "28b58b4a-37d9-470c-ac6e-36ecf11fa079"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(stocks, sectors, how='inner',on='Ticker')\n",
    "df.drop(columns='Name', inplace=True)\n",
    "stocks = df.copy()\n",
    "stocks['Utility'] = np.where(stocks.Sector == 'Utilities',1,0)\n",
    "stocks.loc[stocks.Sector == 'Utilities',].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-cylinder",
   "metadata": {
    "id": "tamil-fault"
   },
   "source": [
    "#### Time trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-report",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moved-adapter",
    "outputId": "5108241a-132c-4250-8302-b38ac89fc425"
   },
   "outputs": [],
   "source": [
    "stocks['time_trend'] = stocks.groupby('Ticker').cumcount()\n",
    "stocks.loc[stocks.Ticker == 'XOM'].groupby(['Ticker','Year']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-maryland",
   "metadata": {},
   "source": [
    "### K-fold cross validation function\n",
    "\n",
    "Below, we created a function called 'kfold_cross_validation' where we send the number of splits, X and y values and the model_type. The function creates the number of k-folds and fits and make predictions for each split. \n",
    "\n",
    "\n",
    "At the end, the function returns the Mean Squared error, the Root Mean Square Error and the Coefficient of determination (R2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation(n_splits, X, y, model_type):\n",
    "    data_y, data_yhat,coef = [], [],[]\n",
    "    kfold = KFold(n_splits=n_splits, random_state = 42, shuffle=True)\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # get data\n",
    "        train_X, test_X = X[train_ix], X[test_ix]\n",
    "        train_y, test_y = y[train_ix], y[test_ix]\n",
    "        # fit model\n",
    "        model = model_type\n",
    "        model.fit(train_X, train_y)\n",
    "        # make predictions\n",
    "        yhat = model.predict(test_X)\n",
    "            \n",
    "        coef.append(model.coef_)\n",
    "        # store\n",
    "        data_y.extend(test_y)\n",
    "        data_yhat.extend(yhat)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print('Mean squared error: %.2f' % metrics.mean_squared_error(data_y, data_yhat))\n",
    "    print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(data_y, data_yhat))}')\n",
    "    print('Coefficient of determination: %.2f'% metrics.r2_score(data_y, data_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-comedy",
   "metadata": {
    "id": "chief-baghdad"
   },
   "source": [
    "## Models\n",
    "\n",
    "### Model 1 - Sales and Assets\n",
    "\n",
    "\n",
    "GHG = a + b1*Sales + b2*Assets + e     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-syndicate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XpB3CW8ZW1h",
    "outputId": "4644994b-d74f-479a-9caa-eb9bf537f3fe"
   },
   "outputs": [],
   "source": [
    "df=stocks.copy()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-scottish",
   "metadata": {},
   "source": [
    "For this model, the predictors we are interested in are Total Assets and Total Sales and the outcome variable is GHG Scope 1. We will proceed to subset the dataframe to get these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-baker",
   "metadata": {
    "id": "xwVmIqfAaQC4"
   },
   "outputs": [],
   "source": [
    "da=df[['GHG Scope 1','Total_Sales', 'Total_Assets']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-receipt",
   "metadata": {},
   "source": [
    "We are now interested in the observations that are not missing GHG Scope. As presented in the previous notebooks, we have a lot of observations missing the GHG Scope 1 value. Therefore, from 780 observations we only get 398 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-april",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "jlC_zRkDc-_n",
    "outputId": "258f5769-c496-4be3-a768-b53b17952271"
   },
   "outputs": [],
   "source": [
    "dn=da.dropna()\n",
    "print(f'Now, we have {dn.shape[0]} observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-philadelphia",
   "metadata": {},
   "source": [
    "### Validation set approach - Hold out\n",
    "\n",
    "Now, we create the X and y variables for the predictors and outcome variable, respectively. Also, we split the dataset into train (80%) and test (20%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-channel",
   "metadata": {
    "id": "bca7cLlTdsez"
   },
   "outputs": [],
   "source": [
    "X = dn.iloc[:, 1:3].values\n",
    "y = dn.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-dynamics",
   "metadata": {
    "id": "generic-surrey"
   },
   "outputs": [],
   "source": [
    "print(f'When we do not include any explanatory variables, the GHG Scope 1 is {round(regressor.intercept_,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-conference",
   "metadata": {},
   "source": [
    "Now, we are going to predict the GHG Scope using the test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-india",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "2xH0qzXjHWHX",
    "outputId": "bc995d26-7712-49a4-f67e-8c6d363e3896"
   },
   "outputs": [],
   "source": [
    "y_train_pred = regressor.predict(X_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-stuart",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQ6AzDEGKDBl",
    "outputId": "c3f7552f-29d0-4138-b83b-742cbc8e7982"
   },
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE train: %.3f, test: %.3f' % (metrics.mean_squared_error(y_train, y_train_pred),\n",
    "                metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-giving",
   "metadata": {},
   "source": [
    "The model is overfitting, it performs slightly better in train but poorly in test. We can apply other ML techniques to work with the overfitting and play with the Bias-Variance tradeoff in order to get a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-saturn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-ZcnvyxKtgE",
    "outputId": "d1264dec-88bf-43a8-a259-e5e7528f6292"
   },
   "outputs": [],
   "source": [
    "r2 =   metrics.r2_score(y_test, y_pred)\n",
    "print('R^2: {0}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-beverage",
   "metadata": {},
   "source": [
    "As presented, the MSE is high which means our model isn't great and is overfitting. Moreover, this model explains around 55% of the variance in our dataset. Next, we will create a 3D visualization to present the predictions for this multiple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train ,columns=['Sales','Assets'])\n",
    "df['GHG_Scope1']= pd.Series(y_train)\n",
    "\n",
    "x_surf, y_surf = np.meshgrid(np.linspace(df.Assets.min(), df.Assets.max(), 100),np.linspace(df.Sales.min(), df.Sales.max(), 100))\n",
    "\n",
    "onlyX = pd.DataFrame({'Sales': x_surf.ravel(), 'Assets': y_surf.ravel()})\n",
    "fittedY= regressor.predict(onlyX)\n",
    "fittedY=np.array(fittedY)\n",
    "\n",
    "fig = plt.figure(figsize=(25,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df.Assets,df.Sales,df.GHG_Scope1,c='red', marker='o', alpha=0.5)\n",
    "ax.plot_surface(x_surf,y_surf,fittedY.reshape(x_surf.shape), color='b', alpha=0.3)\n",
    "ax.set_xlabel('Total Sales')\n",
    "ax.set_ylabel('Total Assets')\n",
    "ax.set_zlabel('GHG Scope 1')\n",
    "fig.suptitle('Linear Regression - Sales and Assets vs. GHG Scope 1', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-prague",
   "metadata": {},
   "source": [
    "Also, we can visualize the observed vs predictions to get a better idea of the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(y_test, y_pred, c='green')\n",
    "ax.plot([y_test.min(), y_test.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Observed GHG Scope 1')\n",
    "ax.set_ylabel('Predicted GHG Scope 1')\n",
    "ax.set_title('Linear Regression- Hold out approach');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-boating",
   "metadata": {},
   "source": [
    "### K-fold cross validation\n",
    "\n",
    "Now, we will apply another cross validation technique to improve the model predictions. We first did 10 k-folds and then 5 - k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dn.iloc[:, 1:3].values\n",
    "y = dn.iloc[:, 0].values\n",
    "regressor = LinearRegression()\n",
    "kfold_cross_validation(10,X,y,regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "kfold_cross_validation(5,X,y,regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-present",
   "metadata": {},
   "source": [
    "10 k-fold cross validation presents a higher MSE than 5-fold cross validation. Both of them explained 46% of the variance which is lower than Linear regression - hold out approach. \n",
    "\n",
    "We will create a visualization to present the predictions for Linear Regression using 5-kfold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "X = dn.iloc[:, 1:3].values\n",
    "y = dn.iloc[:, 0].values\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(y, predicted, c='red')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Observed GHG Scope 1')\n",
    "ax.set_ylabel('Predicted GHG Scope 1')\n",
    "ax.set_title('5-kfold cross-validation predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-contributor",
   "metadata": {},
   "source": [
    "As presented, the model does not fit the data well. The model accounts for 46% of the variance. Let's remember that the more variance that is accounted for by the regression model the closer the data points will fall to the fitted regression line. The visualization presents big residuals between the observed and the fitted GHG Scope 1. \n",
    "\n",
    "We will proceed to add this results to a dataframe where we can keep track of all the models and their respective MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval = pd.DataFrame({'Model_Number':1,'Model_Name': 'Linear (Sales + Asset)','Approach':'Hold-out','MSE': metrics.mean_squared_error(y_test, y_pred), 'RMSE' :np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 'R-Squared' :  metrics.r2_score(y_test, y_pred)}, index=[0])\n",
    "df_model_eval.loc[len(df_model_eval.index)] = [2, 'Linear (Sales + Asset)', '5 K-fold', 572243005.35, 23921.601228835698, 0.46] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-friendship",
   "metadata": {},
   "source": [
    "### Ridge Regression \n",
    "\n",
    "#### Validation set approach (Hold-out)\n",
    "\n",
    "We started with 100 alphas and instead of arbitrarily choosing alpha, we used cross-validation to choose the tuning parameter alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "\n",
    "ridge_1 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge_1.fit(X_train, y_train)\n",
    "yhat_ridge = ridge_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_ridge))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-consciousness",
   "metadata": {},
   "source": [
    "#### K-fold cross validation\n",
    "\n",
    "\n",
    "Now, we are going to use 5 and 10 k-fold cross validation to try to yield a lower MSE. We continued using the Ridge Regression with the best alpha but now using the K-fold cross validation to fit and predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_2 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "kfold_cross_validation(5,X,y,ridge_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_3 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "kfold_cross_validation(10,X,y,ridge_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [3, 'Ridge (Sales + Asset)', '5 K-fold', 574765481.16, 23974.267, 0.46] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_3 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "predicted = cross_val_predict(ridge_3, X, y, cv=5)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(25,10));\n",
    "fig.suptitle('Ridge Regression - Total Assets and Total Sales vs. GHG Scope 1');\n",
    "sns.scatterplot(x=y_test, y = yhat_ridge, ax=axs[0], color='b');\n",
    "axs[0].set_title('Validation Set Approach - Hold out');\n",
    "axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4);\n",
    "sns.scatterplot(x=y, y = predicted, ax=axs[1], color='g');\n",
    "axs[1].set_title('5-kfold cross validation');\n",
    "axs[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-jerusalem",
   "metadata": {},
   "source": [
    "Both models don't fit the data well. The model on the left accounts for 55% of the variance and the model on the right accounts for 46% of variance. Again, the more variance that is accounted for by the regression model the closer the data points will fall to the fitted regression line.\n",
    "\n",
    "We can say that the model on the left is better but we need to take into consideration that it has less data points (Presenting Testing 20% of the data). On the other hand, it presents all the data points and the fitted line. It accounts for less variance but the RMSE decreases too. \n",
    "\n",
    "What do we prefer? Validation Set approach because it accounts for more variance and it will behave better with unseen data, right?\n",
    "\n",
    "Why a flexible model isn't working that well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-sample",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "#### Validation Set approach - Hold out\n",
    "\n",
    "First, we will plot the relationship between alpha and the weights (regression parameters), a line for each features. In this case, two (Sales and Assets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "alphas = 10**np.linspace(6,-2,100)*0.5\n",
    "\n",
    "X = dn.iloc[:, 1:3].values\n",
    "y = dn.iloc[:, 0].values\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "print(np.shape(coefs))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-porter",
   "metadata": {},
   "source": [
    "Now, we are going to subset into training and test and calculate the best optimal alpha using cross validation. (Tuning the hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print(f'The optimal alpha is {lassocv.alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "yhat_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_lasso))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_lasso))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_lasso ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [4, 'Lasso (Sales + Assets)', 'Hold-out',metrics.mean_squared_error(y_test,yhat_lasso), np.sqrt(metrics.mean_squared_error(y_test,yhat_lasso)), metrics.r2_score(y_test, yhat_lasso)] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-casting",
   "metadata": {},
   "source": [
    "#### K-fold Cross validation approach\n",
    "\n",
    "Now, we are going to use k folds to fit and predict. We are going to continue using the best optimal alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "kfold_cross_validation(5, X, y, lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "kfold_cross_validation(10, X, y, lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "predicted = cross_val_predict(lasso, X, y, cv=5)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(25,10));\n",
    "fig.suptitle('Lasso Regression - Total Assets and Total Sales vs. GHG Scope 1');\n",
    "sns.scatterplot(x=y_test, y = yhat_lasso, ax=axs[0], color='b');\n",
    "axs[0].set_title('Validation Set Approach - Hold out');\n",
    "axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4);\n",
    "sns.scatterplot(x=y, y = predicted, ax=axs[1], color='g');\n",
    "axs[1].set_title('5-kfold cross validation');\n",
    "axs[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [5, 'Lasso (Sales + Assets)', '5-kfold',572164056.06, 23919.95, 0.46] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-visitor",
   "metadata": {
    "id": "educated-engineering"
   },
   "source": [
    "### Model 2 - Logarithm Sales and Assets\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "#### Validation Set Approach - Hold out\n",
    "\n",
    "ln(GHG) = a0 + b10*ln(Sales) + b20*ln(Assets) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-basketball",
   "metadata": {
    "id": "better-regulation"
   },
   "outputs": [],
   "source": [
    "model_df = stocks[['Total_Sales', 'Total_Assets', 'GHG Scope 1']].copy()\n",
    "model_df.dropna(inplace = True)\n",
    "\n",
    "x = np.log(model_df[['Total_Sales', 'Total_Assets']])\n",
    "y = np.log(model_df['GHG Scope 1'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "clf = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.exp(clf.predict(X_test))\n",
    "y_test = np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-answer",
   "metadata": {
    "id": "touched-boston",
    "outputId": "b5762f30-e7d1-4d1a-ea3a-dc2c0b3a2583"
   },
   "outputs": [],
   "source": [
    "#The coefficients\n",
    "print('Coefficients: \\n',clf.coef_)\n",
    "#Mean Squared Error\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test,y_pred))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test,y_pred))}')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = pd.DataFrame({'Observed':y_test,'Prediction':y_pred})\n",
    "log_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(log_reg.Observed, log_reg.Prediction, c='red')\n",
    "ax.plot([log_reg.min(), log_reg.max()], [log_reg.min(), log_reg.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Observed GHG Scope 1')\n",
    "ax.set_ylabel('Predicted GHG Scope 1')\n",
    "ax.set_title('Log(Total Assets) and Log (Total Sales) vs. GHG Scope 1 - Linear Regression Hold out');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-photograph",
   "metadata": {},
   "source": [
    "This model accounts for 40% of the variance. Similar as the other regressions, as soon the GHG Scope starts to increase, the model starts making bad predictions (high bias, way out of target). \n",
    "\n",
    "We will proceed to add this results to the dataframe and continue to apply other ML techniques to try to yield a lower MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [6, 'Linear (ln(Sales) + ln(Asset))', 'Hold-out',metrics.mean_squared_error(y_test,y_pred), np.sqrt(metrics.mean_squared_error(y_test,y_pred)), metrics.r2_score(y_test, y_pred)] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-brief",
   "metadata": {},
   "source": [
    "#### K-fold cross validation\n",
    "\n",
    "Now, we will apply another cross validation technique to improve the model predictions. First, we will do 5 k-folds and then 10 - k folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = stocks.loc[stocks['GHG Scope 1'].notna(),['GHG Scope 1','Logarithm_Total_Sales','Logarithm_Total_Assets']].copy()\n",
    "X = df1.iloc[:,1:3].values\n",
    "y = df1.iloc[:,0].values\n",
    "regressor = LinearRegression()\n",
    "kfold_cross_validation(5,X,y,regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.iloc[:,1:3].values\n",
    "y = df1.iloc[:,0].values\n",
    "regressor = LinearRegression()\n",
    "kfold_cross_validation(10,X,y,regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "df_kfold = pd.DataFrame({'Observed':y.flatten(), 'Predicted': predicted.flatten()})\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(y, predicted, c='red')\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Observed GHG Scope 1')\n",
    "ax.set_ylabel('Predicted GHG Scope 1')\n",
    "ax.set_title('5-kfold cross-validation predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [7 , 'Linear (ln(Sales) + ln(Asset))', '5-kfold',676789732.54,26015.18, 0.36] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-screen",
   "metadata": {},
   "source": [
    "### Ridge Regression \n",
    "\n",
    "#### Validation Set Approach - Hold out approach\n",
    "\n",
    "Similar as model 1, we will continue applying different ML techniques to yield the lower MSE. First, we will go with Ridge Regression and apply the Hold Out approach which means we will use 80% for training and 20% for testing. \n",
    "\n",
    "\n",
    "We started with 100 alphas and instead of arbitrarily choosing alpha, we will use cross-validation to choose the tuning parameter alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "X = df1.iloc[:,1:3].values\n",
    "y = df1.iloc[:,0].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "\n",
    "ridge_1 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge_1.fit(X_train, y_train)\n",
    "yhat_ridge = ridge_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Coefficients:', ridge_1.coef_)\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_ridge))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-saying",
   "metadata": {},
   "source": [
    "#### K-fold validation\n",
    "\n",
    "We applied cross validation to get the best alpha. However, now we are going to do k-fold cross validation to apply to train and test the data in different folds. \n",
    "\n",
    "We continued using the Ridge Regression with the best alpha but now using the K-fold cross validation to fit and predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_2 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "kfold_cross_validation(5,X,y,ridge_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_2 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "kfold_cross_validation(10,X,y,ridge_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X,y)\n",
    "ridge_3 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "predicted = cross_val_predict(ridge_3, X, y, cv=5)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(25,10));\n",
    "fig.suptitle('Ridge Regression - ln(Total Sales) and ln(Total Assets) vs. GHG Scope 1');\n",
    "sns.scatterplot(x=y_test, y = yhat_ridge, ax=axs[0], color='b');\n",
    "axs[0].set_title('Validation Set Approach - Hold out');\n",
    "axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4);\n",
    "sns.scatterplot(x=y, y = predicted, ax=axs[1], color='g');\n",
    "axs[1].set_title('5-kfold cross validation');\n",
    "axs[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [8 , 'Ridge (ln(Sales) + ln(Asset))', 'Hold-out',metrics.mean_squared_error(y_test,yhat_ridge), np.sqrt(metrics.mean_squared_error(y_test,yhat_ridge)), metrics.r2_score(y_test, yhat_ridge)] \n",
    "df_model_eval.loc[len(df_model_eval.index)] = [9 , 'Ridge (ln(Sales) + ln(Asset))', '5-kfolds',676843560.22, 26016.21, 0.36] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-hands",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "#### Validation Set Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "alphas = 10**np.linspace(6,-2,100)*0.5\n",
    "\n",
    "X = df1.iloc[:,1:3].values\n",
    "y = df1.iloc[:,0].values\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print(f'The optimal alpha is {lassocv.alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "yhat_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_lasso))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_lasso))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_lasso ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-persian",
   "metadata": {},
   "source": [
    "#### K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "kfold_cross_validation(5, X, y, lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "kfold_cross_validation(10, X, y, lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "predicted = cross_val_predict(lasso, X, y, cv=5)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(25,10));\n",
    "fig.suptitle('Lasso Regression - ln(Total Sales) and ln(Total Assets) vs. GHG Scope 1');\n",
    "sns.scatterplot(x=y_test, y = yhat_lasso, ax=axs[0], color='b');\n",
    "axs[0].set_title('Validation Set Approach - Hold out');\n",
    "axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4);\n",
    "sns.scatterplot(x=y, y = predicted, ax=axs[1], color='g');\n",
    "axs[1].set_title('5-kfold cross validation');\n",
    "axs[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [10 , 'Lasso (ln(Sales) + ln(Asset))', 'Hold-out',metrics.mean_squared_error(y_test,yhat_ridge), np.sqrt(metrics.mean_squared_error(y_test,yhat_ridge)), metrics.r2_score(y_test, yhat_ridge)] \n",
    "df_model_eval.loc[len(df_model_eval.index)] = [11 , 'Lasso (ln(Sales) + ln(Asset))', '5-kfolds',677276647.87, 26024.53, 0.36] \n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-advantage",
   "metadata": {
    "id": "vital-blowing"
   },
   "source": [
    "### Model 3 -  GHG for Energy vs Utilities\n",
    "\n",
    "GHG = a + b1*Sales + b2*Assets + b3*Util + b5*TimeTrend + e\n",
    "\n",
    "#### Linear Regression - Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-session",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "freelance-maple",
    "outputId": "7779c7a4-5dbc-44e1-bcb2-1b15c4d494ff"
   },
   "outputs": [],
   "source": [
    "stock = stocks.copy()\n",
    "stock.dropna(inplace=True, subset = ['Total_Assets','Total_Sales','GHG Scope 1'])\n",
    "\n",
    "X = stock[['Total_Sales','Total_Assets','Utility','time_trend']]\n",
    "y = stock['GHG Scope 1']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-secretary",
   "metadata": {
    "id": "capable-fitting"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) #Initialize model\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42) #Train and Test split\n",
    "model.fit(Xtrain,ytrain) #Fit the model\n",
    "y_model = model.predict(Xtest) #Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-textbook",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eleven-chart",
    "outputId": "79190a11-8f7c-4724-a0b8-5380df9b80e8"
   },
   "outputs": [],
   "source": [
    "#The coefficients\n",
    "print('Coefficients: \\n', model.coef_)\n",
    "#Mean Squared Error\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(ytest,y_model))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(ytest,y_model))}')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % metrics.r2_score(ytest, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-exchange",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cardiac-check",
    "outputId": "cccb11c0-f09b-4927-bf06-037ec13c2151"
   },
   "outputs": [],
   "source": [
    "#Create data frame with observed and predicted\n",
    "linear_reg = pd.DataFrame({'Observed':ytest,'Prediction':y_model})\n",
    "linear_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [12, 'Linear Regression (Sales + Asset + Utilities + Timetrend)', 'Hold-out', metrics.mean_squared_error(ytest,y_model), np.sqrt(metrics.mean_squared_error(ytest,y_model)), metrics.r2_score(ytest, y_model)]\n",
    "df_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(X_train ,columns=['Sales','Assets','Utilities','Timetrend'])\n",
    "# df['GHG_Scope1']= pd.Series(y_train)\n",
    "\n",
    "# x_surf, y_surf, z_surf, zz_surf = np.meshgrid(np.linspace(df.Sales.min(), df.Sales.max(), 100),\n",
    "#                                      np.linspace(df.Assets.min(), df.Assets.max(), 100),\n",
    "#                                     np.linspace(df.Utilities.min(), df.Utilities.max(), 100),\n",
    "#                                     np.linspace(df.Timetrend.min(), df.Timetrend.max(), 100))\n",
    "\n",
    "# onlyX = pd.DataFrame({'Sales': x_surf.ravel(), 'Assets': y_surf.ravel(), \n",
    "#                       'Utilities': z_surf.ravel(), 'Timetrend': zz_surf.ravel()})\n",
    "# fittedY= regressor.predict(onlyX)\n",
    "# fittedY=np.array(fittedY)\n",
    "\n",
    "# fig = plt.figure(figsize=(25,12))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(df.Sales,df.Assets,df.Utilities, df.Timetrend, df.GHG_Scope1,c='red', marker='o', alpha=0.5)\n",
    "# ax.plot_surface(x_surf,y_surf,z_surf, zz_surf, fittedY.reshape(x_surf.shape), color='b', alpha=0.3)\n",
    "# ax.set_xlabel('Total Sales')\n",
    "# ax.set_ylabel('Total Assets')\n",
    "# ax.set_zlabel('GHG Scope 1')\n",
    "# fig.suptitle('Linear Regression - Sales and Assets vs. GHG Scope 1', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-chassis",
   "metadata": {
    "id": "married-uzbekistan"
   },
   "source": [
    "#### Linear regression with k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-employer",
   "metadata": {
    "id": "later-basketball"
   },
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-documentary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efficient-upset",
    "outputId": "dddb1017-2406-4188-aaed-f4b936581a32"
   },
   "outputs": [],
   "source": [
    "model1 = LinearRegression()\n",
    "kfold_cross_validation(10,X,y,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-happiness",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "former-heart",
    "outputId": "113e8beb-34bf-4897-cabd-9e2d6d6c07ea"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5,X,y,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = [4, 'Linear Regression CV 5K-fold (Sales + Asset + Utilities + Timetrend)', 458669164.20,  21416.56 ,0.57 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-today",
   "metadata": {
    "id": "tender-combat"
   },
   "source": [
    "#### Ridge Regression - Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-plasma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "minor-boutique",
    "outputId": "3b5b9f0b-7ae9-4bfb-e22d-9b8bb4f7674b"
   },
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a, fit_intercept=False, normalize=True)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ridge.fit(Xtrain, ytrain)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-tourist",
   "metadata": {
    "id": "heard-quality"
   },
   "source": [
    "Do we need to standarized the data? One feature is staying high.. would it look better if we normalize it? Would the logarithm work better because it distributes the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-thanksgiving",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "explicit-yield",
    "outputId": "e30ecef4-731d-4d39-947c-b71510ff0b23"
   },
   "outputs": [],
   "source": [
    "coefs[:5] #First five coefficients\n",
    "#print(f'Last 5 coefficients: {coefs[(len(coefs) - 5):len(coefs)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-myanmar",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "friendly-clinic",
    "outputId": "4d91caa0-a376-45d7-92bd-42cd8976ac41"
   },
   "outputs": [],
   "source": [
    "coefs[(len(coefs) - 5):len(coefs)] #the last 5 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-white",
   "metadata": {
    "id": "hungarian-conjunction"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-electric",
   "metadata": {
    "id": "parliamentary-finance"
   },
   "source": [
    "Instead of arbitrarily choosing alpha, we used cross-validation to choose the tuning parameter alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-dining",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "impressive-network",
    "outputId": "76b18a2f-5ac7-4e0f-93f9-b9f803bf4ab9"
   },
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-montreal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "southern-kingdom",
    "outputId": "d4966857-0b7b-49f1-c889-3974776f66c7"
   },
   "outputs": [],
   "source": [
    "ridge_1 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge_1.fit(X_train, y_train)\n",
    "yhat_ridge = ridge_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_ridge))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Ridge (Sales + Asset + Utilities + Timetrend)', metrics.mean_squared_error(y_test, yhat_ridge), np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge)), metrics.r2_score(y_test, yhat_ridge)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-disorder",
   "metadata": {
    "id": "liable-season"
   },
   "source": [
    "#### Ridge Regression - Cross validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-preparation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "contrary-relaxation",
    "outputId": "22e02287-e56d-4ffa-ee0d-44782e5ed40e"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(10,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-indonesia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "portuguese-mentor",
    "outputId": "abb8dd28-8362-4ce3-9404-2ff5b3239f0d"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-baseball",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tropical-geography",
    "outputId": "993ff8d0-b927-4b92-e749-a905d60168a6"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(2,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Ridge CV 5K-fold (Sales + Asset + Utilities + Timetrend)', 458487743.03, 21412.326894264337 ,0.57 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-welcome",
   "metadata": {
    "id": "removed-canal"
   },
   "source": [
    "#### Lasso - Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-wesley",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "broke-basin",
    "outputId": "e29f2d77-a1bb-4347-9339-e9da99ba86e6"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-communication",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seventh-empty",
    "outputId": "9aaff7a8-6574-495a-86af-347ae6352c79"
   },
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print(f'The optimal alpha is {lassocv.alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-south",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raising-packing",
    "outputId": "155b6b8c-7484-481e-c6b6-ebe7d239308b"
   },
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "yhat_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_lasso))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_lasso))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_lasso ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Lasso (Sales + Asset + Utilities + Timetrend)', metrics.mean_squared_error(y_test, yhat_lasso), np.sqrt(metrics.mean_squared_error(y_test, yhat_lasso)),metrics.r2_score(y_test, yhat_lasso ) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-wholesale",
   "metadata": {
    "id": "broadband-penny"
   },
   "source": [
    "#### Lasso Regression - Cross validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-discussion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dense-register",
    "outputId": "58f1cb89-0c94-4cd7-9e47-10e105ecb05b"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(10, X,y,lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-reverse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "effective-occasions",
    "outputId": "cec736a0-d16f-4f22-af92-513730405744"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5, X,y,lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Lasso CV 5K-fold (Sales + Asset + Utilities + Timetrend)', 458655027.23, 21416.23279729868 ,0.57 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-joshua",
   "metadata": {
    "id": "suburban-client"
   },
   "source": [
    "### Model 4 -  GHG for Energy vs Utilities using natural logarithms\n",
    "\n",
    "\n",
    "ln(GHG) = a + b1*ln(Sales) + b2*ln(Assets) + b3*ln(Util) + b5*ln(TimeTrend) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-friendship",
   "metadata": {
    "id": "favorite-launch"
   },
   "outputs": [],
   "source": [
    "X=stock[['Logarithm_Total_Sales','Logarithm_Total_Assets','Utility','time_trend']]\n",
    "y=stock['GHG Scope 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-fountain",
   "metadata": {
    "id": "zlj4QO1ACsWh"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) #Initialize model\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42) #Train and Test split\n",
    "model.fit(Xtrain,ytrain) #Fit the model\n",
    "y_model = model.predict(Xtest) #Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-board",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grtFHqCVC5F0",
    "outputId": "19618acb-6556-4350-b4fa-3401ddf91b92"
   },
   "outputs": [],
   "source": [
    "#The coefficients\n",
    "print('Coefficients: \\n', model.coef_)\n",
    "#Mean Squared Error\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(ytest,y_model))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(ytest,y_model))}')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % metrics.r2_score(ytest, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-patient",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWjno9mxC-p-",
    "outputId": "5339e445-243c-4aae-895e-f83a33838e25"
   },
   "outputs": [],
   "source": [
    "#Create data frame with observed and predicted\n",
    "linear_reg4 = pd.DataFrame({'Observed':ytest,'Prediction':y_model})\n",
    "linear_reg4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Logarithm (Sales + Asset + Utilities + Timetrend)', metrics.mean_squared_error(ytest, y_model),np.sqrt(metrics.mean_squared_error(ytest, y_model)), metrics.r2_score(ytest, y_model)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-detection",
   "metadata": {
    "id": "ek-ET5WvDfuD"
   },
   "source": [
    "#### Linear regression with k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-sculpture",
   "metadata": {
    "id": "wBn3-wbvDfuE"
   },
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "def kfold_cross_validation(n_splits, X, y, model_type):\n",
    "    data_y, data_yhat,coef = [], [],[]\n",
    "    kfold = KFold(n_splits=n_splits, random_state = 42, shuffle=True)\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # get data\n",
    "        train_X, test_X = X[train_ix], X[test_ix]\n",
    "        train_y, test_y = y[train_ix], y[test_ix]\n",
    "        # fit model\n",
    "        model = model_type\n",
    "        model.fit(train_X, train_y)\n",
    "        # make predictions\n",
    "        yhat = model.predict(test_X)\n",
    "        coef.append(model.coef_)\n",
    "        # store\n",
    "        data_y.extend(test_y)\n",
    "        data_yhat.extend(yhat)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print('Mean squared error: %.2f' % metrics.mean_squared_error(data_y, data_yhat))\n",
    "    print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(data_y, data_yhat))}')\n",
    "    print('Coefficient of determination: %.2f'% metrics.r2_score(data_y, data_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-borough",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYbPrV2_DfuE",
    "outputId": "b7ee6cfa-d107-4010-e578-bc72187f1e63"
   },
   "outputs": [],
   "source": [
    "model1 = LinearRegression()\n",
    "kfold_cross_validation(10,X,y,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-knife",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmw48BVyDfuF",
    "outputId": "2feba5fe-74cb-46db-91e1-f9f97c58faeb"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5,X,y,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Logarithm Linear 5-kfold (Sales + Asset + Utilities + Timetrend)', 569033682.28,23854.42689056714,0.46 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-premiere",
   "metadata": {
    "id": "UZ8r9R7cEAmG"
   },
   "source": [
    "#### Ridge Regression - Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-vanilla",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63OKRhv_EAmH",
    "outputId": "84767c24-5b13-4422-9500-cb3a63ee3ba0"
   },
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a, fit_intercept=False, normalize=True)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ridge.fit(Xtrain, ytrain)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-remedy",
   "metadata": {
    "id": "vnItdngXEAmI"
   },
   "source": [
    "Do we need to standarized the data? One feature is staying high.. would it look better if we normalize it? Would the logarithm work better because it distributes the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-castle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEFBf8HoEAmI",
    "outputId": "0813e1ae-099c-4448-e199-158790bbf623"
   },
   "outputs": [],
   "source": [
    "coefs[:5] #First five coefficients\n",
    "#print(f'Last 5 coefficients: {coefs[(len(coefs) - 5):len(coefs)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-budget",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQlnyJ_wEAmI",
    "outputId": "a6fcaeae-5d77-4918-bd54-f9b09ea6a680"
   },
   "outputs": [],
   "source": [
    "coefs[(len(coefs) - 5):len(coefs)] #the last 5 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-hunter",
   "metadata": {
    "id": "t4uYQBq0EAmJ"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-ratio",
   "metadata": {
    "id": "TmSgenc-EAmJ"
   },
   "source": [
    "Instead of arbitrarily choosing alpha, we used cross-validation to choose the tuning parameter alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-toyota",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkNRy60lEAmJ",
    "outputId": "60d119ef-f389-4bb9-ed5c-cc300aa4490b"
   },
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-tunnel",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHoA36aUEAmK",
    "outputId": "33a9f290-ddc7-4c6b-9baf-de7c7bc96af0"
   },
   "outputs": [],
   "source": [
    "ridge_1 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge_1.fit(X_train, y_train)\n",
    "yhat_ridge = ridge_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_ridge))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Logarithm Ridge (Sales + Asset + Utilities + Timetrend)', metrics.mean_squared_error(y_test, yhat_ridge), np.sqrt(metrics.mean_squared_error(y_test, yhat_ridge)), metrics.r2_score(ytest, yhat_ridge)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-breakfast",
   "metadata": {
    "id": "EC68yCjVFu20"
   },
   "source": [
    "#### Ridge Regression - Cross validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-chapter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Z0x4YTXFu21",
    "outputId": "b23de2d4-9f6b-45d5-d27d-27b6607e4178"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(10,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-sensitivity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bm5X0bFRFu22",
    "outputId": "2a0e6d75-574c-4e0b-ce59-bc3d22b70171"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-internship",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAnnZq_HFu22",
    "outputId": "fa1fde6c-9502-4a9d-cf8a-e6a155b4a230"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(2,X,y,ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Logarithm Ridge 5-kfold (Sales + Asset + Utilities + Timetrend)', 569122863.08,23856.29,0.46 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-symbol",
   "metadata": {
    "id": "XVOkxPjiF-8j"
   },
   "source": [
    "#### Lasso - Validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-laser",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvWUf7D2F-8k",
    "outputId": "0075be8e-9818-4b76-b04c-1ada6e0860ce"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-davis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgjioWj4F-8n",
    "outputId": "94af4f99-ab79-42af-f3ef-4fc0bde5d443"
   },
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print(f'The optimal alpha is {lassocv.alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-corporation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ovl17a07F-8n",
    "outputId": "90fb98a7-c2a9-4d3a-baf4-64c28694722c"
   },
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "yhat_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, yhat_lasso))\n",
    "print(f'Root Mean Squared Error: {np.sqrt(metrics.mean_squared_error(y_test, yhat_lasso))}')\n",
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, yhat_lasso ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-belize",
   "metadata": {
    "id": "AV2fr0KfGHQj"
   },
   "source": [
    "#### Lasso Regression - Cross validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-rendering",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGzdw10sGHQj",
    "outputId": "9dd4e783-aa5b-409a-b603-bb23b3f539b6"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(10, X,y,lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-aside",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIhbwy_jGHQk",
    "outputId": "928e1608-15db-41c1-c0f8-42c8be59b5be"
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation(5, X,y,lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval.loc[len(df_model_eval.index)] = ['Logarithm Lasso 5-kfold (Sales + Asset + Utilities + Timetrend)', 569052835.46,23854.83,0.46 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-delivery",
   "metadata": {
    "id": "divine-glass"
   },
   "source": [
    "### Conclusion - Model performance\n",
    "\n",
    "All four predictive models had a big RSME when predicting the GHG Scope. Model 3 - had the lower RSME and the best R2 with 0.66.\n",
    "\n",
    "However, we need to continue with other approach to work with the model performance or consider other ways to impute the missing values for GHG Scope 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_eval[['Model','RMSE','R-Squared']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(25,8.27)})\n",
    "g = sns.barplot('Model', 'MSE', data = df_model_eval)\n",
    "for item in g.get_xticklabels():\n",
    "    item.set_rotation(10)\n",
    "    \n",
    "g.figure.savefig('Model MSEs.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(25,8.27)})\n",
    "g = sns.barplot('Model', 'R-Squared', data = df_model_eval)\n",
    "for item in g.get_xticklabels():\n",
    "    item.set_rotation(10)\n",
    "    \n",
    "g.figure.savefig('Model R-Squared.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sprint4_Model_Evaluation （4 Models）.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
